<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Neural Networks</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/dracula.css">
	<link rel="stylesheet" href="https://unpkg.com/tldreveal/dist/bundle/index.css" />


	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/styles/atom-one-dark.css">
</head>
<body>
<div class="reveal">
	<div class="slides">
		<section>
			<h2>Artificial Neural Networks</h2>
			<p>Aftab Khan<p>
		</section>
		<section>
			<section>
				<h2>Binary Classification</h2>
				<p>Categorize data points in two classes.</p>
			</section>
			<section >
				<h2>Linear Classifiers</h2>
				<p>
					\[y = m \cdot x + c\]
				</p>
				<div style="width: 1100px; display: flex; justify-content: space-around">
					<div data-style="width: 500px" class="fig-container" data-file="./static/plots/linear-classification.html"></div>
					<div style="padding-top: 100px">
						<p>
							\[
							m = \text{slope}
							\]
						</p>
						<p>
							\[
							c = \text{intercept}
							\]
						</p>
					</div>
				</div>
			</section>
		</section>

		<section>
			<section>
				<h1>XOR Problem</h1>
			</section>
			<section >
				<h2 style="margin-bottom: 50px">XOR Binary Function</h2>
				\[
				f(x,y) =
				\begin{cases}
				1 & \text{if } x \neq y \\
				0 & \text{otherwise}
				\end{cases}
				\]
				<p>\[\text{for} \ x,y \in \{0,1\} \]</p>
			</section>
			<section>
				<h2>XOR Classification</h2>
				<p>But, can we classify the XOR using linear classifiers ?</p>
				<div class="fig-container" data-file="./static/plots/xor-plot.html"></div>
			</section>
		</section>
		<section>
			<section>
				<h2>Artificial Neural Networks</h2>
			</section>
			<section>
				<h2>McCulloch-Pitts Neuron (1943)</h2>
				<ul>
					<li>Inspired from a biological neuron.</li>
					<li> Binary output based on inputs.</li>
					<li> Activates on a threshold.</li>
				</ul>
				<p>
					\[
					y = \Theta(\sum_{i=1}^{n} w_i x_i)
					\]
				</p>
				<p >
					\[
					w_i = \text{weights} \quad x_i = \text{inputs} \quad \Theta = \text{threshold}
					\]
				</p>
			</section>
			<section>
				<h2>Biological Inspiration</h2>
				<div style="display: flex; padding-top: 30px; width: 100%; justify-content: space-between">
					<img style="height: 300px ;width: auto" src="static/images/Neuron.svg" alt="Biological Neuron">
					<img style="height: 300px ;width: auto" src="static/images/ArtificialNeuron.svg" alt="Artificial Neuron">
				</div>
			</section>
		</section>
		<section>
			<section>
				<!-- <div style="font-size: large; color:lightslategrey; border: solid 1px grey; border-radius: 8px; position: fixed; top: 0; padding: 2px 4px;" >
                1958
                </div> -->
				<h2>
					Perceptron (1958)
				</h2>
				<ul>
					<li>Introduced by Frank Rosenblatt .</li>
					<li>Practical application of McCulloch-Pitts neurons.</li>
				</ul>
				<p>
					\[
					y = \varphi(\sum_{i=1}^{n} w_i x_i + b)
					\]
				</p>

				<p >
					\[
					\quad b = \text{bias} \quad \varphi = \text{activation function}
					\]
				</p>
			</section>
			<section data-auto-animate>
				<h2>
					Can it classify XOR?
				</h2>
				<!-- show that linear classifier and perceptron are mathematically the same -->

			</section>
			<section data-auto-animate>
				<h2>
					Can it classify XOR?
				</h2>
				<!-- show that linear classifier and perceptron are mathematically the same -->
				<p>
					\[
					y = \varphi(\sum_{i=1}^{n} w_i x_i + b) \simeq (m \cdot x + c)
					\]
				</p>
				<p style="color: hotpink; font-weight: bold">Still Linear !</p>

			</section>
			<section>
				<h2>
					AI Winter (1970-1980)
				</h2>

				<ul>
					<li>Perceptron cannot solve non-linearly separable problems</li>
					<li>Lack of computational power</li>
					<li>No funding for AI research</li>
				</ul>

			</section>

		</section>
		<section>
			<section>
				<h2>
					Maybe add more layers?
				</h2>
			</section>
			<section data-auto-animate>
				<h2>
					Multilayer Perceptron
				</h2>
				<div >
					<img src="static/images/Neural_network.svg" alt="Multilayer Perceptron" style="height: 400px; width: auto">
				</div>
			</section>
			<section data-auto-animate>
				<h2>
					Multilayer Perceptron
				</h2>
				<ul>
					<li>Stack multiple perceptrons together.</li>
					<li>Introduces hidden layers.</li>
					<li>Adds non-linearity using <span style="font-weight: bold; color: #2ad7dc">activation functions</span>.</li>
					<li>Uses
						<span style="font-weight: bold; color: #2ad7dc">backpropagation</span> for training.
					</li>
				</ul>
			</section>
		</section>
		<section>
			<section>
				<h2>
					Activation Function
				</h2>
				<ul>
					<li>Adds non-linearity to the model.
						<span class="fragment" style="color: #2ad7dc; font-weight: bold">but how ?</span>
					</li>
					<li>Eg. ReLU, Sigmoid, Tanh, etc.</li>
				</ul>

				\[
				ReLU(x) =
				\begin{cases}
				x & \text{if } x > 0 \\
				0 & \text{otherwise}
				\end{cases}
				\]
			</section>

			<section  class="fig-container" data-style="overflow:hidden;"
					  data-file="https://www.desmos.com/calculator/x29rguen5v"

			>
			</section>
		</section>

		<section>
			<section>
				<h2>
					Backpropagation (1982)
				</h2>
				<ul>
					<li>Introduced by Rumelhart, Hinton and Williams.</li>
					<li>Optimizes the weights of the network.</li>
					<li>Uses the
						<span style="color: #2ad7dc; font-weight: bold">derivatives</span> and
						<span style="color: #2ad7dc; font-weight: bold">chain rule</span> of calculus.
					</li>
				</ul>
			</section>

			<!--
            <section>

                <h2>
                    Derivative of a Function
                </h2>
                <p>Rate of change of a function at a point.</p>
                <div class="fig-container" data-file="./static/plots/derivative.html"></div>


            </section>
            -->
			<section>
				<h2>
					Derivative of a Function
				</h2>
				<p>It tells us how small changes in the input affect the output.</p>
				\[
				\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
				\]

			</section>
			<section data-auto-animate>
				<h2>
					Chain Rule
				</h2>
				<p>If bicycle rider is twice as fast as the person walking, and a car is twice as fast as the bicycle, then the car is four times as fast as the person walking.</p>

				\[
				\frac{\partial \text{car}}{\partial \text{person}} = \frac{\partial \text{car}}{\partial \text{bicycle}} \cdot \frac{\partial \text{bicycle}}{\partial \text{person}}
				\]
			</section>
			<section data-auto-animate>
				<h2>
					Chain Rule
				</h2>
				<p>If bicycle rider is twice as fast as the person walking, and a car is twice as fast as the bicycle, then the car is four times as fast as the person walking.</p>

				\[
				\frac{\partial f(g(x))}{\partial x} = \frac{\partial f(g(x))}{\partial g(x)} \cdot \frac{\partial g(x)}{\partial x}
				\]
			</section>
		</section>
		<section>
			<section>
				<h2>Training Process</h2>
			</section>
			<section>
				<h2>5-Steps Training</h2>
				<ol>
					<li>Initialize the weights randomly.</li>
					<li>Forward pass to get the output.</li>
					<li>Calculate the loss.</li>
					<li>Backpropagate the gradients.</li>
					<li>Update the weights.</li>
				</ol>
			</section>
			<section>
				<h2>Initialization</h2>
				<p>Randomly initialize the weights of the network.</p>
				<p style="font-size: larger">
					\[
					w = \text{random}
					\]
				</p>
			</section>
			<section>
				<h2>Forward Pass</h2>
				<p>Pass the input through the network to get the output.</p>
				<p style="font-size: larger">
					\[
					y_{\text{pred}} = f(w \cdot x)
					\]
				</p>

				\[
				f = \text{activation function}
				\]


			</section>
			<section>
				<h2>Loss Calculation</h2>
				<p>Calculate the difference between the predicted and actual output.</p>
				\[
				\text{Loss} = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{pred}_i} - y_{\text{true}_i})^2
				\]

				\[ \ n = \text{number of samples} \]

			</section>
			<section>
				<h2>Backpropagation</h2>
				<p>Calculate the gradients of the loss with respect to the weights.</p>
				\[
				\frac{\partial \text{Loss}}{\partial w} = \frac{\partial \text{Loss}}{\partial y_{\text{pred}}} \cdot \frac{\partial y_{\text{pred}}}{\partial w}
				\]

			</section>
			<section>
				<h2>Update Parameters</h2>
				<p>Update the weights using the gradients.</p>
				\[
				w_{\text{new}} = w_{\text{old}} - \alpha \cdot \frac{\partial \text{Loss}}{\partial w}
				\]

				\[
				\alpha = \text{learning rate}
				\]
			</section>
			<section>
				<h2>In PyTorch</h2>
				<pre>
							<code data-trim data-noescape data-line-numbers="10-14" >
								base_model = torch.nn.Sequential(
						        torch.nn.Linear(num_facts, 128),
						        torch.nn.ReLU(),
						        torch.nn.Linear(128, num_actions)
						    )

								num_epochs = 200
								for i in range(0, num_epochs):

									training_model.train() // set model to training mode
									y = training_model(inputs) // forward pass
									loss = lossfn(y, expected_outputs) // calculate loss
									loss.backward() // backpropagate
									optimizer.step() // update weights

							</code>
						</pre>
			</section>
		</section>
		<section>
			<section>
				<h2>
					Can it now solve the XOR problem ?
					<span style="color: #2ad7dc; font-weight: bold" class="fragment">
							Yes!
						</span>
				</h2>
			</section>
			<section>
				<div class="fig-container" style="margin-top: -220px; overflow-y : hidden" data-scroll="no"
					 data-file="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.92734&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&discretize_hide=true&regularization_hide=true&batchSize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&noise_hide=true&problem_hide=true&showTestData_hide=true&activation_hide=true"
				></div>
			</section>
		</section>
		<section>
			<h3>
				Universal Approximation Theorem (1989)
			</h3>
			<blockquote>
				"A feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of \( \mathbb{R}^n \), under mild assumptions on the activation function."
				<sup>
					<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">[1]</a>
				</sup>
			</blockquote>

		</section>
		<section>
			<h2>
				Current Advancements
			</h2>
			<ul>
				<li>Deep Learning</li>
				<li>Convolutional Neural Networks</li>
				<li>Recurrent Neural Networks</li>
				<li>Generative Adversarial Networks</li>
			</ul>
		</section>
		<section>
			<h2> Nature-Inspired ? </h2>
			<table class="fragment">
				<tr>
					<th>Humans</th>
					<th>Machines</th>
				</tr>
				<tr>
					<td class="fragment">Some perception about a thing</td>
					<td class="fragment">Random weights</td>
				</tr>
				<tr>
					<td class="fragment">Trying it</td>
					<td class="fragment">Forward pass</td>
				</tr>
				<tr>
					<td class="fragment">Realising mistakes</td>
					<td class="fragment">Backpropagation</td>
				</tr>
				<tr>
					<td class="fragment">Changing the perception</td>
					<td class="fragment">Update weights</td>
				</tr>
				<tr>
					<td class="fragment">Can never be perfect</td>
					<td class="fragment">?</td>
				</tr>

			</table>
		</section>
		<section>
			<h2>
				Thank You!
			</h2>
		</section>
		<section>
			<h2>References and Resources</h2>
			<ul>
				<li>
					<a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Wikipedia - Artificial Neural Networks</a>
				</li>
				<li>
					<a href="https://en.wikipedia.org/wiki/Perceptron">Wikipedia - Perceptron</a>
				</li>
				<li>
					<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Wikipedia - Universal Approximation Theorem</a>
				</li>
				<li>
					<a href="https://en.wikipedia.org/wiki/Backpropagation">Wikipedia - Backpropagation</a>
				</li>
				<li>
					<a href="https://www.desmos.com/calculator/x29rguen5v">Desmos - Activation Functions</a>
				</li>
				<li>
					<a href="https://playground.tensorflow.org/">Tensorflow Playground</a>
				</li>
				<li>
					<a href="https://www.youtube.com/watch?v=SQeZ2FONQEw">YouTube - Universal Approximation Theorem</a>
				</li>
			</ul>
		</section>
	</div>
</div>

<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script src="plugin/math/math.js"></script>
<script src="https://unpkg.com/tldreveal/dist/bundle/index.js"></script>
<script src="plugin/reveald3.js"></script>


<script>
	// More info about initialization & config:
	// - https://revealjs.com/initialization/
	// - https://revealjs.com/config/
	Reveal.initialize({
		hash: true,
		width: 1200,
		slideNumber: 'c/t',


		// Learn about plugins: https://revealjs.com/plugins/
		plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX, Tldreveal.Tldreveal(), Reveald3 ]
	});
</script>
</body>
</html>